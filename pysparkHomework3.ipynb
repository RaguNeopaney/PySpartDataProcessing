{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97cc5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import size\n",
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b6d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e11ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb5be11b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365\n"
     ]
    }
   ],
   "source": [
    "# 1st 2010 \n",
    "data2010_1 = file_dir + \"/2010/01052099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2010_1)\n",
    "csv_df.createOrReplaceTempView(\"data_1_2010sql\")\n",
    "\n",
    "# 2nd 2010\n",
    "data2010_2 = file_dir + \"/2010/99407099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2010_2)\n",
    "csv_df.createOrReplaceTempView(\"data_2_2010sql\")\n",
    "\n",
    "# 1st 2011\n",
    "data2011_1 = file_dir + \"/2011/01008099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2011_1)\n",
    "csv_df.createOrReplaceTempView(\"data_1_2011sql\")\n",
    "\n",
    "\n",
    "# 2nd 2011\n",
    "data2011_2 = file_dir + \"/2011/01046099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2011_2)\n",
    "csv_df.createOrReplaceTempView(\"data_2_2011sql\")\n",
    "\n",
    "# 1st 2012\n",
    "data2012_1 = file_dir + \"/2012/01023099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2012_1)\n",
    "csv_df.createOrReplaceTempView(\"data_1_2012sql\")\n",
    "\n",
    "# 2nd 2012\n",
    "data2012_2 = file_dir + \"/2012/01044099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2012_2)\n",
    "csv_df.createOrReplaceTempView(\"data_2_2012sql\")\n",
    "\n",
    "# 1st 2013\n",
    "data2013_1 = file_dir + \"/2013/01001499999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2013_1)\n",
    "csv_df.createOrReplaceTempView(\"data_1_2013sql\")\n",
    "\n",
    "# 2nd 2013\n",
    "data2013_2 = file_dir + \"/2013/01008099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2013_2)\n",
    "csv_df.createOrReplaceTempView(\"data_2_2013sql\")\n",
    "\n",
    "# 1st 2014\n",
    "data2014_1 = file_dir + \"/2014/01008099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2014_1)\n",
    "csv_df.createOrReplaceTempView(\"data_1_2014sql\")\n",
    "\n",
    "# 2nd 2014\n",
    "data2014_2 = file_dir + \"/2014/01023099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2014_2)\n",
    "csv_df.createOrReplaceTempView(\"data_2_2014sql\")\n",
    "\n",
    "# 1st 2015\n",
    "data2015_1 = file_dir + \"/2015/01008099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2015_1)\n",
    "print(csv_df.count())\n",
    "csv_df.createOrReplaceTempView(\"data_1_2015sql\")\n",
    "\n",
    "# 2nd 2015\n",
    "data2015_2 = file_dir + \"/2015/01025099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2015_2)\n",
    "csv_df.createOrReplaceTempView(\"data_2_2015sql\")\n",
    "\n",
    "# 1st 2016\n",
    "data2016_1 = file_dir + \"/2016/01008099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2016_1)\n",
    "csv_df.createOrReplaceTempView(\"data_1_2016sql\")\n",
    "\n",
    "# 2nd 2016\n",
    "data2016_2 = file_dir + \"/2016/01023199999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2016_2)\n",
    "csv_df.createOrReplaceTempView(\"data_2_2016sql\")\n",
    "\n",
    "# 1st 2017\n",
    "data2017_1 = file_dir + \"/2017/01008099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2017_1)\n",
    "csv_df.createOrReplaceTempView(\"data_1_2017sql\")\n",
    "\n",
    "# 2nd 2017\n",
    "data2017_2 = file_dir + \"/2017/01023099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2017_2)\n",
    "csv_df.createOrReplaceTempView(\"data_2_2017sql\")\n",
    "\n",
    "# 1st 2018\n",
    "data2018_1 = file_dir + \"/2018/01008099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2018_1)\n",
    "csv_df.createOrReplaceTempView(\"data_1_2018sql\")\n",
    "\n",
    "# 2nd 2018\n",
    "data2018_2 = file_dir + \"/2018/01025099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2018_2)\n",
    "csv_df.createOrReplaceTempView(\"data_2_2018sql\")\n",
    "\n",
    "# 1st 2019\n",
    "data2019_1 = file_dir + \"/2019/01008099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2019_1)\n",
    "csv_df.createOrReplaceTempView(\"data_1_2019sql\")\n",
    "\n",
    "# 2nd 2019\n",
    "data2019_2 = file_dir + \"/2019/01023099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2019_2)\n",
    "csv_df.createOrReplaceTempView(\"data_2_2019sql\")\n",
    "\n",
    "# 1st 2020\n",
    "data2020_1 = file_dir + \"/2020/01008099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2020_1)\n",
    "csv_df.createOrReplaceTempView(\"data_1_2020sql\")\n",
    "\n",
    "# 2nd 2020\n",
    "data2020_2 = file_dir + \"/2020/01023099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2020_2)\n",
    "csv_df.createOrReplaceTempView(\"data_2_2020sql\")\n",
    "\n",
    "# 1st 2021\n",
    "data2021_1 = file_dir + \"/2021/01062099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2021_1)\n",
    "csv_df.createOrReplaceTempView(\"data_1_2021sql\")\n",
    "\n",
    "# 2nd 2021\n",
    "data2021_2 = file_dir + \"/2021/01065099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2021_2)\n",
    "csv_df.createOrReplaceTempView(\"data_2_2021sql\")\n",
    "\n",
    "# 1st 2022\n",
    "data2022_1 = file_dir + \"/2022/01241099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2022_1)\n",
    "csv_df.createOrReplaceTempView(\"data_1_2022sql\")\n",
    "\n",
    "# 2nd 2022\n",
    "data2022_2 = file_dir + \"/2022/02095099999.csv\"\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data2022_2)\n",
    "csv_df.createOrReplaceTempView(\"data_2_2022sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f38be9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       STATION                       NAME        DATE Max Temp\n",
      "0  01052099999     HAMMERFEST AIRPORT, NO  2010-06-26     72.9\n",
      "0  99407099999  DESTRUCTION IS. WA, WA US  2010-08-15     74.8\n",
      "0  01008099999               LONGYEAR, SV  2011-08-17     62.8\n",
      "0  01046099999              SORKJOSEN, NO  2011-07-09     87.8\n",
      "0  01023099999              BARDUFOSS, NO  2012-07-05     72.0\n",
      "0  01044099999                 HASVIK, NO  2012-07-14     69.8\n",
      "0  01001499999             SORSTOKKEN, NO  2013-08-02     80.6\n",
      "0  01008099999               LONGYEAR, SV  2013-07-03     59.0\n",
      "1  01008099999               LONGYEAR, SV  2013-08-17     59.0\n",
      "0  01008099999               LONGYEAR, SV  2014-07-08     53.6\n",
      "1  01008099999               LONGYEAR, SV  2014-08-03     53.6\n",
      "2  01008099999               LONGYEAR, SV  2014-08-04     53.6\n",
      "3  01008099999               LONGYEAR, SV  2014-08-22     53.6\n",
      "0  01023099999              BARDUFOSS, NO  2014-07-10     89.6\n",
      "0  01008099999               LONGYEAR, SV  2015-07-30     64.4\n",
      "0  01025099999                 TROMSO, NO  2015-07-30     71.6\n",
      "0  01008099999               LONGYEAR, SV  2016-07-03     58.1\n",
      "0  01023199999                DRAUGEN, NO  2016-07-21     77.0\n",
      "0  01008099999               LONGYEAR, SV  2017-07-17     55.4\n",
      "0  01023099999              BARDUFOSS, NO  2017-06-09     78.6\n",
      "0  01008099999               LONGYEAR, SV  2018-08-02     59.2\n",
      "0  01025099999                 TROMSO, NO  2018-07-29     84.2\n",
      "0  01008099999               LONGYEAR, SV  2019-07-06     61.0\n",
      "0  01023099999              BARDUFOSS, NO  2019-07-21     78.8\n",
      "1  01023099999              BARDUFOSS, NO  2019-07-22     78.8\n",
      "0  01008099999               LONGYEAR, SV  2020-07-25     71.1\n",
      "0  01023099999              BARDUFOSS, NO  2020-06-22     79.9\n",
      "0  01062099999           HOPEN ISLAND, NO  2021-10-05     47.3\n",
      "0  01065099999               KARASJOK, NO  2021-07-05     88.3\n",
      "0  01241099999                 ORLAND, NO  2022-06-25     82.4\n",
      "1  01241099999                 ORLAND, NO  2022-06-26     82.4\n",
      "2  01241099999                 ORLAND, NO  2022-07-01     82.4\n",
      "0  02095099999                 PAJALA, SW  2022-07-01     85.5\n"
     ]
    }
   ],
   "source": [
    "# calculates Max temp and puts it into pandas dataframe so it is easier to write on output file\n",
    "df_sql_1 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_1_2010sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_1_2010sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_2 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_2_2010sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_2_2010sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_3 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_1_2011sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_1_2011sql WHERE MAX < 9999.9 )\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_4 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_2_2011sql WHERE MAX = (SELECT MAX(float(MAX))FROM data_2_2011sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_5 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_1_2012sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_1_2012sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_6 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_2_2012sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_2_2012sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_7 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_1_2013sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_1_2013sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_8 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_2_2013sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_2_2013sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_9 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_1_2014sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_1_2014sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_10 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_2_2014sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_2_2014sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_11 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_1_2015sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_1_2015sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_12 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_2_2015sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_2_2015sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_13 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_1_2016sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_1_2016sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_14 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_2_2016sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_2_2016sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_15 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_1_2017sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_1_2017sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_16 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_2_2017sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_2_2017sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_17 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_1_2018sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_1_2018sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_18 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_2_2018sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_2_2018sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_19 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_1_2019sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_1_2019sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_20 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_2_2019sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_2_2019sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_21 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_1_2020sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_1_2020sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_22 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_2_2020sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_2_2020sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_23 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_1_2021sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_1_2021sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_24 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_2_2021sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_2_2021sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_25 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_1_2022sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_1_2022sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "df_sql_26 = spark.sql(\"SELECT STATION,NAME,DATE,MAX FROM data_2_2022sql WHERE MAX = (SELECT MAX(float(MAX)) FROM data_2_2022sql WHERE MAX < 9999.9)\").withColumnRenamed(\"MAX\", \"Max Temp\").toPandas()\n",
    "\n",
    "masterDfForMaxTemp = pd.concat([df_sql_1,df_sql_2,df_sql_3,df_sql_4,df_sql_5,df_sql_6,df_sql_7,df_sql_8,df_sql_9,df_sql_10,df_sql_11,df_sql_12,df_sql_13,df_sql_14,df_sql_15,df_sql_16,df_sql_17,df_sql_18,df_sql_19,df_sql_20,df_sql_21,df_sql_22,df_sql_23,df_sql_24,df_sql_25,df_sql_26],  axis=0)\n",
    "print(masterDfForMaxTemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adee6aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        STATION                       NAME        DATE Min Temp\n",
      "0   01052099999     HAMMERFEST AIRPORT, NO  2010-01-29     -1.1\n",
      "1   99407099999  DESTRUCTION IS. WA, WA US  2010-11-23     25.2\n",
      "2   01008099999               LONGYEAR, SV  2011-01-30    -23.8\n",
      "3   01046099999              SORKJOSEN, NO  2011-02-08     -5.8\n",
      "4   01023099999              BARDUFOSS, NO  2012-02-07    -22.5\n",
      "5   01044099999                 HASVIK, NO  2012-02-06      3.2\n",
      "6   01001499999             SORSTOKKEN, NO  2013-03-11     17.6\n",
      "7   01008099999               LONGYEAR, SV  2013-03-05    -15.2\n",
      "8   01008099999               LONGYEAR, SV  2014-12-27     -4.7\n",
      "9   01023099999              BARDUFOSS, NO  2014-01-12    -18.9\n",
      "10  01008099999               LONGYEAR, SV  2015-02-11    -16.6\n",
      "11  01025099999                 TROMSO, NO  2015-01-12      6.3\n",
      "12  01008099999               LONGYEAR, SV  2016-12-08     -0.4\n",
      "13  01023199999                DRAUGEN, NO  2016-01-08     19.4\n",
      "14  01008099999               LONGYEAR, SV  2017-03-18    -10.3\n",
      "15  01023099999              BARDUFOSS, NO  2017-01-05    -28.3\n",
      "16  01008099999               LONGYEAR, SV  2018-12-27     -5.8\n",
      "17  01025099999                 TROMSO, NO  2018-03-10      5.7\n",
      "18  01008099999               LONGYEAR, SV  2019-02-18    -13.0\n",
      "19  01023099999              BARDUFOSS, NO  2019-01-31    -22.0\n",
      "20  01023099999              BARDUFOSS, NO  2019-11-11    -22.0\n",
      "21  01008099999               LONGYEAR, SV  2020-03-11    -22.0\n",
      "22  01023099999              BARDUFOSS, NO  2020-01-27    -19.3\n",
      "23  01062099999           HOPEN ISLAND, NO  2021-03-16     -8.9\n",
      "24  01062099999           HOPEN ISLAND, NO  2021-03-17     -8.9\n",
      "25  01065099999               KARASJOK, NO  2021-12-05    -32.1\n",
      "26  01241099999                 ORLAND, NO  2022-02-02     17.2\n",
      "27  02095099999                 PAJALA, SW  2022-01-08    -28.1\n"
     ]
    }
   ],
   "source": [
    "# calculates Min temp and puts it into pandas dataframe so it is easier to write on output file\n",
    "df_sql_1_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_1_2010sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_1_2010sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "df_sql_2_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_2_2010sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_2_2010sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = df_sql_1_min.union(df_sql_2_min)\n",
    "df_sql_3_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_1_2011sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_1_2011sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_3_min)\n",
    "df_sql_4_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_2_2011sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_2_2011sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_4_min)\n",
    "df_sql_5_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_1_2012sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_1_2012sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_5_min)\n",
    "df_sql_6_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_2_2012sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_2_2012sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_6_min)\n",
    "df_sql_7_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_1_2013sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_1_2013sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_7_min)\n",
    "df_sql_8_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_2_2013sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_2_2013sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_8_min)\n",
    "df_sql_9_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_1_2014sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_1_2014sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_9_min)\n",
    "df_sql_10_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_2_2014sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_2_2014sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_10_min)\n",
    "df_sql_11_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_1_2015sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_1_2015sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_11_min)\n",
    "df_sql_12_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_2_2015sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_2_2015sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_12_min)\n",
    "df_sql_13_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_1_2016sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_1_2016sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_13_min)\n",
    "df_sql_14_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_2_2016sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_2_2016sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_14_min)\n",
    "df_sql_15_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_1_2017sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_1_2017sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_15_min)\n",
    "df_sql_16_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_2_2017sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_2_2017sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_16_min)\n",
    "df_sql_17_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_1_2018sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_1_2018sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_17_min)\n",
    "df_sql_18_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_2_2018sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_2_2018sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_18_min)\n",
    "df_sql_19_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_1_2019sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_1_2019sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_19_min)\n",
    "df_sql_20_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_2_2019sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_2_2019sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_20_min)\n",
    "df_sql_21_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_1_2020sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_1_2020sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_21_min)\n",
    "df_sql_22_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_2_2020sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_2_2020sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_22_min)\n",
    "df_sql_23_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_1_2021sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_1_2021sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_23_min)\n",
    "df_sql_24_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_2_2021sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_2_2021sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_24_min)\n",
    "df_sql_25_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_1_2022sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_1_2022sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_25_min)\n",
    "df_sql_26_min = spark.sql(\"SELECT STATION,NAME,DATE,MIN FROM data_2_2022sql WHERE MIN = (SELECT Min(float(MIN)) FROM data_2_2022sql)\").withColumnRenamed(\"MIN\", \"Min Temp\")\n",
    "unionDF = unionDF.union(df_sql_26_min)\n",
    "\n",
    "masterDfForMinTemp = unionDF.toPandas()\n",
    "print(masterDfForMinTemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c23ea253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX PRCP\n",
      "       STATION          NAME        DATE Max PRCP\n",
      "0  01008099999  LONGYEAR, SV  2015-12-19     0.72\n",
      "1  01025099999    TROMSO, NO  2015-11-02     2.11\n",
      "MIN PRCP\n",
      "         STATION        NAME        DATE Min PRCP\n",
      "0    01025099999  TROMSO, NO  2015-01-04     0.00\n",
      "1    01025099999  TROMSO, NO  2015-01-05     0.00\n",
      "2    01025099999  TROMSO, NO  2015-01-06     0.00\n",
      "3    01025099999  TROMSO, NO  2015-01-07     0.00\n",
      "4    01025099999  TROMSO, NO  2015-01-09     0.00\n",
      "..           ...         ...         ...      ...\n",
      "363  01025099999  TROMSO, NO  2015-12-18     0.00\n",
      "364  01025099999  TROMSO, NO  2015-12-20     0.00\n",
      "365  01025099999  TROMSO, NO  2015-12-29     0.00\n",
      "366  01025099999  TROMSO, NO  2015-12-30     0.00\n",
      "367  01025099999  TROMSO, NO  2015-12-31     0.00\n",
      "\n",
      "[368 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Maximum and Minimum precipitation (column PRCP ) for the year 2015,  and provide the \n",
    "# corresponding station code, station name and the date (columns STATION, NAME, DATE).\n",
    "# MAX PRCP\n",
    "df_sql_1_max_prcp = spark.sql(\"SELECT STATION,NAME,DATE,PRCP FROM data_1_2015sql WHERE PRCP = (SELECT Max(float(PRCP)) FROM data_1_2015sql WHERE PRCP < 99.99)\").withColumnRenamed(\"PRCP\", \"Max PRCP\")\n",
    "df_sql_2_max_prcp = spark.sql(\"SELECT STATION,NAME,DATE,PRCP FROM data_2_2015sql WHERE PRCP = (SELECT Max(float(PRCP)) FROM data_2_2015sql WHERE PRCP < 99.99)\").withColumnRenamed(\"PRCP\", \"Max PRCP\")\n",
    "unionDF = df_sql_1_max_prcp.union(df_sql_2_max_prcp)\n",
    "masterDfForMaxPRCP = unionDF.toPandas()\n",
    "\n",
    "# MIN \n",
    "df_sql_min_prcp = spark.sql(\"SELECT STATION,NAME,DATE,PRCP FROM data_1_2015sql WHERE PRCP = (SELECT Min(float(PRCP)) FROM data_1_2015sql)\").withColumnRenamed(\"PRCP\", \"Min PRCP\")\n",
    "df_sql_min_prcp = spark.sql(\"SELECT STATION,NAME,DATE,PRCP FROM data_2_2015sql WHERE PRCP = (SELECT Min(float(PRCP)) FROM data_2_2015sql)\").withColumnRenamed(\"PRCP\", \"Min PRCP\")\n",
    "unionDF = df_sql_min_prcp.unionByName(df_sql_min_prcp)\n",
    "masterDfForMinPRCP = unionDF.toPandas()\n",
    "\n",
    "print('MAX PRCP')\n",
    "print(masterDfForMaxPRCP)\n",
    "print('MIN PRCP')\n",
    "print(masterDfForMinPRCP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3d7fc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing wind gust percentage for 2019 file 01008099999.csv: 83.84 %\n",
      "missing wind gust percentage for 2019 file 01023099999.csv: 81.92 %\n",
      "missing wind gust percentage for 2019 both file combined: 82.88 %\n"
     ]
    }
   ],
   "source": [
    "# Count percentage missing values for wind gust (column GUST) for the year 2019.\n",
    "df_sql_1_missing_Gust = spark.sql(\"SELECT GUST FROM data_1_2019sql WHERE float(GUST) > 997.9\")\n",
    "df_sql_2_missing_Gust = spark.sql(\"SELECT GUST FROM data_2_2019sql WHERE float(GUST) > 997.9\")\n",
    "unionDF = df_sql_1_missing_Gust.union(df_sql_2_missing_Gust)\n",
    "\n",
    "df_sql_1_all_value = spark.sql(\"SELECT GUST FROM data_1_2019sql\").count()\n",
    "df_sql_2_all_value = spark.sql(\"SELECT GUST FROM data_2_2019sql\").count()\n",
    "\n",
    "percentage_first_file_missing_gust = '{0:.4g}'.format((((df_sql_1_missing_Gust).count())/df_sql_1_all_value)*100) \n",
    "percentage_second_file_missing_gust = '{0:.4g}'.format(((((df_sql_2_missing_Gust).count())/df_sql_2_all_value)*100))\n",
    "percentage_total_2019_missing_gust = '{0:.4g}'.format((unionDF.count() / (df_sql_1_all_value+df_sql_2_all_value)) * 100)\n",
    "\n",
    "\n",
    "print(f'missing wind gust percentage for 2019 file 01008099999.csv: {percentage_first_file_missing_gust} %')\n",
    "print(f'missing wind gust percentage for 2019 file 01023099999.csv: {percentage_second_file_missing_gust} %')\n",
    "print(f'missing wind gust percentage for 2019 both file combined: {percentage_total_2019_missing_gust} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3658d80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        STATION          NAME      Month   Mean Median  Mode STD Dev.\n",
      "0   01008099999  LONGYEAR, SV    January  8.016    5.7   5.7     9.09\n",
      "1   01008099999  LONGYEAR, SV   February  4.141    2.8   2.8    9.946\n",
      "2   01008099999  LONGYEAR, SV      March      3    0.5  18.6    12.17\n",
      "3   01008099999  LONGYEAR, SV      April  16.01   19.5   N/A    13.72\n",
      "4   01008099999  LONGYEAR, SV        May  32.29   35.1   N/A    7.743\n",
      "5   01008099999  LONGYEAR, SV       June  40.26  38.95   N/A    3.712\n",
      "6   01008099999  LONGYEAR, SV       July  49.61   47.9   N/A    5.733\n",
      "7   01008099999  LONGYEAR, SV     August  45.11   44.7   N/A    4.455\n",
      "8   01008099999  LONGYEAR, SV  September  37.93  37.65  31.8    4.276\n",
      "9   01008099999  LONGYEAR, SV    October  28.05     27   N/A    5.751\n",
      "10  01008099999  LONGYEAR, SV   November  26.92   27.1   N/A    7.347\n",
      "11  01008099999  LONGYEAR, SV   December  19.45   19.4   N/A    9.048\n",
      "        STATION           NAME      Month   Mean Median  Mode STD Dev.\n",
      "0   01023099999  BARDUFOSS, NO    January  23.78   25.7   N/A    11.07\n",
      "1   01023099999  BARDUFOSS, NO   February  22.58   22.4  15.5    8.602\n",
      "2   01023099999  BARDUFOSS, NO      March  26.31   26.1   N/A    8.826\n",
      "3   01023099999  BARDUFOSS, NO      April  30.65  33.05   N/A    6.759\n",
      "4   01023099999  BARDUFOSS, NO        May  40.15   37.6  37.0    6.387\n",
      "5   01023099999  BARDUFOSS, NO       June   54.6  53.75   N/A    6.339\n",
      "6   01023099999  BARDUFOSS, NO       July  56.16   56.4   N/A    5.939\n",
      "7   01023099999  BARDUFOSS, NO     August  53.46   54.8   N/A    5.593\n",
      "8   01023099999  BARDUFOSS, NO  September  45.76   45.1   N/A    4.531\n",
      "9   01023099999  BARDUFOSS, NO    October     35     36  23.2    11.39\n",
      "10  01023099999  BARDUFOSS, NO   November  31.57   31.6  31.6    8.351\n",
      "11  01023099999  BARDUFOSS, NO   December  20.46   21.2   N/A    8.776\n"
     ]
    }
   ],
   "source": [
    "# Find the mean, median, mode and standard deviation of the Temperature (column TEMP) for \n",
    "# each month for the year 2020.\n",
    "\n",
    "name_1 = spark.sql(\"SELECT distinct(NAME) FROM data_1_2020sql\").collect()[0][0]\n",
    "station_1 = spark.sql(\"SELECT distinct(STATION) FROM data_1_2020sql\").collect()[0][0]\n",
    "\n",
    "nameList = [name_1]*12\n",
    "stationList = [station_1] *12\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July',\n",
    "          'August', 'September', 'October', 'November', 'December']\n",
    "Mean = []\n",
    "Median = []\n",
    "Mode = []\n",
    "STDDev = []\n",
    "\n",
    "for i in range(1, 13):\n",
    "    df_sql_1_Temp_2020 = spark.sql(f\"SELECT DATE,TEMP FROM data_1_2020sql WHERE MONTH(DATE) = {i}\")\n",
    "    pandadf = df_sql_1_Temp_2020.toPandas()\n",
    "    pandadf = pandadf.astype({'TEMP': 'float'})\n",
    "    Mean.append('{0:.4g}'.format(pandadf[\"TEMP\"].mean()))\n",
    "    Median.append('{0:.4g}'.format(pandadf[\"TEMP\"].median()))\n",
    "    STDDev.append('{0:.4g}'.format(pandadf[\"TEMP\"].std()))\n",
    "    if(len(pandadf['TEMP'].mode()) == 1):\n",
    "        Mode.append(str(pandadf['TEMP'].mode()[0]))\n",
    "    else:\n",
    "        Mode.append('N/A')\n",
    "\n",
    "\n",
    "data1 = {'STATION': stationList, 'NAME': nameList, 'Month': months, 'Mean': Mean, 'Median' : Median, 'Mode': Mode, 'STD Dev.':STDDev}\n",
    "First_dataframe_stat_calc = pd.DataFrame(data=data1)\n",
    "\n",
    "nameList.clear()\n",
    "stationList.clear()\n",
    "Mean.clear()\n",
    "Median.clear()\n",
    "Mode.clear()\n",
    "STDDev.clear()\n",
    "\n",
    "name_2 = spark.sql(\"SELECT distinct(NAME) FROM data_2_2020sql\").collect()[0][0]\n",
    "station_2 = spark.sql(\"SELECT distinct(STATION) FROM data_2_2020sql\").collect()[0][0]\n",
    "\n",
    "nameList = [name_2]*12\n",
    "stationList = [station_2]*12\n",
    "\n",
    "for i in range(1, 13):\n",
    "    df_sql_2_Temp_2020 = spark.sql(f\"SELECT DATE,TEMP FROM data_2_2020sql WHERE MONTH(DATE) = {i}\")\n",
    "    pandadf = df_sql_2_Temp_2020.toPandas()\n",
    "    pandadf = pandadf.astype({'TEMP': 'float'})\n",
    "    Mean.append('{0:.4g}'.format(pandadf[\"TEMP\"].mean()))\n",
    "    Median.append('{0:.4g}'.format(pandadf[\"TEMP\"].median()))\n",
    "    STDDev.append('{0:.4g}'.format(pandadf[\"TEMP\"].std()))\n",
    "    if(len(pandadf['TEMP'].mode()) == 1):\n",
    "        Mode.append(str(pandadf['TEMP'].mode()[0]))\n",
    "    else:\n",
    "        Mode.append('N/A')\n",
    "\n",
    "data2 = {'STATION': stationList, 'NAME': nameList, 'Month': months, 'Mean': Mean, 'Median' : Median, 'Mode': Mode, 'STD Dev.':STDDev}\n",
    "second_dataframe_stat_calc = pd.DataFrame(data=data2)\n",
    "# TEST AREA\n",
    "print(First_dataframe_stat_calc)\n",
    "print(second_dataframe_stat_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e69c969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hotest day table\n",
    "if os.path.exists(\"result.txt\"):\n",
    "    os.remove(\"result.txt\")\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"1. Find the hottest day (column MAX) for each year, and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE). \\n\\n\")\n",
    "    masterDfForMaxTemp.to_csv('result.txt', header=True, index=False, sep='|', mode='a')\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"\\n2. Find the coldest day (column MIN) for the month of January across all years (2010 - 2022) ,and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE).\\n\\n\")\n",
    "    masterDfForMinTemp.to_csv('result.txt', header=True, index=False, sep='|', mode='a')\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"\\n3. Maximum and Minimum precipitation (column PRCP ) for the year 2015,  and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE).\\nMax PRCP\\n\")\n",
    "    masterDfForMaxPRCP.to_csv('result.txt', header=True, index=False, sep='|', mode='a')\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"\\nMin PRCP\\n\")\n",
    "    masterDfForMinPRCP.to_csv('result.txt', header=True, index=False, sep='|', mode='a')\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"\\n4. Count percentage missing values for wind gust (column GUST) for the year 2019.\\n\")\n",
    "        file_object.write(f\"\\nmissing wind gust percentage for 2019 file 01008099999.csv: {percentage_first_file_missing_gust} %\\n\")\n",
    "        file_object.write(f'missing wind gust percentage for 2019 file 01023099999.csv: {percentage_second_file_missing_gust} %\\n')\n",
    "        file_object.write(f'missing wind gust percentage for 2019 both file combined: {percentage_total_2019_missing_gust} %\\n')\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"\\n5. Find the mean, median, mode and standard deviation of the Temperature (column TEMP) for each month for the year 2020.\\n\")\n",
    "        file_object.write(\"\\nData for year 2020 file 01008099999 \\n\")\n",
    "        file_object.write(\"-----------------------------------------------------------\\n\")\n",
    "    First_dataframe_stat_calc.to_csv('result.txt', header=True, index=False, sep='|', mode='a')\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"\\nData for year 2020 file 01023099999 \\n\")\n",
    "        file_object.write(\"-----------------------------------------------------------\\n\")\n",
    "    second_dataframe_stat_calc.to_csv('result.txt', header=True, index=False, sep='|', mode='a')\n",
    "else:\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"1. Find the hottest day (column MAX) for each year, and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE). \\n\\n\")\n",
    "    masterDfForMaxTemp.to_csv('result.txt', header=True, index=False, sep='|', mode='a')\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"\\n2. Find the coldest day (column MIN) for the month of January across all years (2010 - 2022) ,and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE).\\n\\n\")\n",
    "    masterDfForMinTemp.to_csv('result.txt', header=True, index=False, sep='|', mode='a')\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"\\n3. Maximum and Minimum precipitation (column PRCP ) for the year 2015,  and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE).\\nMax PRCP\\n\")\n",
    "    masterDfForMaxPRCP.to_csv('result.txt', header=True, index=False, sep='|', mode='a')\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"\\nMin PRCP\\n\")\n",
    "    masterDfForMinPRCP.to_csv('result.txt', header=True, index=False, sep='|', mode='a')\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"\\n4. Count percentage missing values for wind gust (column GUST) for the year 2019.\\n\")\n",
    "        file_object.write(f\"\\nmissing wind gust percentage for 2019 file 01008099999.csv: {percentage_first_file_missing_gust} %\\n\")\n",
    "        file_object.write(f'missing wind gust percentage for 2019 file 01023099999.csv: {percentage_second_file_missing_gust} %\\n')\n",
    "        file_object.write(f'missing wind gust percentage for 2019 both file combined: {percentage_total_2019_missing_gust} %\\n')\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"\\n5. Find the mean, median, mode and standard deviation of the Temperature (column TEMP) for each month for the year 2020.\\n\")\n",
    "        file_object.write(\"\\nData for year 2020 file 01008099999 \\n\")\n",
    "        file_object.write(\"-----------------------------------------------------------\\n\")\n",
    "    First_dataframe_stat_calc.to_csv('result.txt', header=True, index=False, sep='|', mode='a')\n",
    "    with open(\"result.txt\", \"a\") as file_object:\n",
    "        file_object.write(\"\\nData for year 2020 file 01023099999 \\n\")\n",
    "        file_object.write(\"-----------------------------------------------------------\\n\")\n",
    "    second_dataframe_stat_calc.to_csv('result.txt', header=True, index=False, sep='|', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929be1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec22342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
